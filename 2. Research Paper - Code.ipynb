{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a4ac93",
   "metadata": {},
   "source": [
    "*Portions of the Python code, including data cleaning, data preparation, and segments of the regression analysis, have been adapted from the Stata code provided in Kaboski and Townsend's paper:*\n",
    "\n",
    "Kaboski, Joseph P., and Robert M. Townsend. \"The Impact of Credit on Village Economies.\" *American Economic Journal: Applied Economics* 4, no. 2 (2012): 98–133. https://doi.org/10.1257/app.4.2.98."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dcb91",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a13083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde, chi2\n",
    "from scipy.signal import find_peaks\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, Ridge, LogisticRegression, lasso_path\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from lightgbm import LGBMRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from econml.dml import CausalForestDML\n",
    "from linearmodels.panel import PanelOLS\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_stata('AnnualData_ShortSample.dta')\n",
    "df.sort_values('case_id', inplace=True)\n",
    "df['tinv'] = df[['hinv', 'finv', 'pinv', 'binv', 'bafdn', 'sinv', 'safdn', 'lvcn', 'lvnn', 'lvdep']].sum(axis=1)\n",
    "df['twage'] = df['frmwage'] + df['shrwage'] + df['buswage']\n",
    "df.drop(columns=['frmwage', 'shrwage', 'buswage'], inplace=True)\n",
    "df['farm'] = ((df['och'] > -5) & (df['och'] <= 15)).astype(int)\n",
    "df['tbinv'] = df[['pinv', 'binv', 'bafdn', 'sinv', 'safdn']].sum(axis=1)\n",
    "df['age2h'] = df['ageh'] ** 2\n",
    "df['gassd2'] = df['gassd'] ** 2\n",
    "df['lgassd'] = np.log(df['gassd'])\n",
    "df['dlgassd1'] = df.groupby('case_id')['gassd'].transform(lambda x: np.log(x.shift(-1) / x))\n",
    "df['dnetinc'] = df.groupby('case_id')['netinc'].transform(lambda x: np.log(x.shift(-1) / x))\n",
    "df['invHH'] = 1 / df['vHH']\n",
    "df.loc[df['year'] == 1, 'invHH'] = df.groupby('case_id')['invHH'].transform(lambda x: x.shift(-1))\n",
    "df['invHHl'] = df.groupby('case_id')['vHH'].transform(lambda x: 1 / x.shift(1))\n",
    "df['vfstl'] = df.groupby('case_id')['vfst'].transform(lambda x: x.shift(1))\n",
    "df['bsnew'] = df['bnew'] + df['snew']\n",
    "\n",
    "# Specify columns to keep\n",
    "columns_to_keep = [\n",
    "    'case_id', 'year', 'frmpro', 'buspro', 'wageinc', 'riceinc', 'cropinc', 'liveinc',\n",
    "    'educ', 'invHHl', 'vfstl', 'invHH', 'changwat', 'amphoe', 'tambon', 'village',\n",
    "    'dnetinc', 'dlgassd1', 'farm', 'netinc', 'bsnew', 'tc', 'educ', 'grain', 'milk',\n",
    "    'meat', 'alch1', 'alch2', 'fuel', 'tobac', 'cerem', 'houserep', 'vehicrep',\n",
    "    'clothes', 'mealaway', 'ageh', 'madult', 'fadult', 'kids', 'maleh', 'finv',\n",
    "    'tbinv', 'hinv', 'frtexp', 'netinc', 'lac', 'newst', 'vfst', 'infst', 'baacst',\n",
    "    'cbst', 'const', 'edust', 'agst', 'hhast', 'busst', 'frtst', 'age2h', 'educh',\n",
    "    'gassd', 'gassd2', 'lgassd', 'rst', 'defcr', 'twage'\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Additional transformations\n",
    "df['villageyear'] = (df['case_id'] // 1000).astype(int)\n",
    "df['vfst'] = df['vfst'] / 10000\n",
    "df['vfstl'] = df['vfstl'] / 10000\n",
    "df.sort_values(by=['case_id', 'year'], inplace=True)\n",
    "df['invHHim'] = np.where(df['year'] == 6, df['invHH'], np.nan)\n",
    "df['invHHi'] = df.groupby('case_id')['invHHim'].transform('mean')\n",
    "df['vHHi'] = 1 / df['invHHi']\n",
    "df = df[(df['vHHi'] <= 250) & (df['vHHi'] >= 50)]\n",
    "df['invHHpvf'] = df['invHHi'] * (df['year'] > 5)\n",
    "df['invHHtvf1'] = df['invHHi'] * (df['year'] == 6)\n",
    "df['invHHtvf2'] = df['invHHi'] * (df['year'] == 7)\n",
    "df['vfstf'] = df['vfst'] * (df['maleh'] == 0)\n",
    "df['invHHtvf1f'] = df['invHHtvf1'] * (df['maleh'] == 0)\n",
    "df['invHHtvf2f'] = df['invHHtvf2'] * (df['maleh'] == 0)\n",
    "df['vfstlf'] = df['vfstl'] * (df['maleh'] == 0)\n",
    "df['caseid'] = df['case_id'].astype(float)\n",
    "df.drop(columns=['case_id'], inplace=True)\n",
    "df['tbinvp'] = df['tbinv'] > 0\n",
    "df['finvp'] = df['finv'] > 0\n",
    "df['defcrp'] = df['defcr'] > 0\n",
    "df = df[df['year'] < 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c647f",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9292729",
   "metadata": {},
   "source": [
    "### General Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ebb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for variable names\n",
    "variable_names = {\n",
    "    'newst': \"New Short-Term Credit\",\n",
    "    'vfst_post_program': \"Village Fund Credit\",\n",
    "    'ageh': \"Age of Household's Head\",\n",
    "    'educh': \"Years of Education of Household's Head\"\n",
    "}\n",
    "\n",
    "# Create 'vfst_post_program' variable\n",
    "df['vfst_post_program'] = df['vfst'].where(df['year'].isin([6, 7]), other=None)\n",
    "\n",
    "# Calculate the mean for specified variables grouped by 'caseid'\n",
    "collapsed_df = df.groupby('caseid')[['newst', 'vfst_post_program', 'ageh', 'educh']].mean()\n",
    "\n",
    "# Scale 'vfst_post_program' for display purposes\n",
    "collapsed_df['vfst_post_program'] *= 10000\n",
    "\n",
    "# Rename columns for readability\n",
    "collapsed_df.rename(columns=variable_names, inplace=True)\n",
    "\n",
    "# Get summary statistics\n",
    "summary_stats_interest = collapsed_df.describe()\n",
    "\n",
    "# Calculate cross-sectional standard deviation\n",
    "cross_sectional_std = collapsed_df.std()\n",
    "\n",
    "# Transpose the summary statistics table and reorder rows\n",
    "summary_stats_transposed = summary_stats_interest.T.reindex([\n",
    "    \"New Short-Term Credit\", \n",
    "    \"Village Fund Credit\",  \n",
    "    \"Age of Household's Head\", \n",
    "    \"Years of Education of Household's Head\"\n",
    "])\n",
    "\n",
    "# Add cross-sectional standard deviation\n",
    "summary_stats_transposed['Cross-Sectional Standard Deviation'] = cross_sectional_std.reindex([\n",
    "    \"New Short-Term Credit\", \n",
    "    \"Village Fund Credit\",  \n",
    "    \"Age of Household's Head\", \n",
    "    \"Years of Education of Household's Head\"\n",
    "])\n",
    "\n",
    "# Calculate counts and proportions for 'farm' dummy variable\n",
    "farm_counts = df['farm'].value_counts().rename({0: 'Non-Farm', 1: 'Farm'})\n",
    "farm_proportions = df['farm'].value_counts(normalize=True).rename({0: 'Non-Farm', 1: 'Farm'}) * 100\n",
    "\n",
    "# Create a summary DataFrame for 'farm'\n",
    "farm_summary = pd.DataFrame({\n",
    "    'Count': farm_counts,\n",
    "    'Proportion (%)': farm_proportions\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dffbc",
   "metadata": {},
   "source": [
    "### Age of Household's Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Extract 'ageh' data and drop NaN values\n",
    "ageh_data = df['ageh'].dropna()\n",
    "\n",
    "# Plot histogram of age with percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(\n",
    "    ageh_data,\n",
    "    bins=20,\n",
    "    kde=False,\n",
    "    color='#4c72b0',\n",
    "    edgecolor='white',\n",
    "    stat=\"percent\"\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Histogram of Age of Household's Head\", fontsize=18, weight='bold', pad=20)\n",
    "plt.xlabel(\"Age of Household's Head\", fontsize=14)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=14)\n",
    "sns.despine()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93018114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create density plot for 'ageh'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(df['ageh'].dropna(), shade=True, color='#4c72b0', lw=2)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Density Plot of Age of Household's Head\", fontsize=18, weight='bold', pad=20)\n",
    "plt.xlabel(\"Age of Household's Head\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "sns.despine()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b145cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'ageh' data and drop NaN values\n",
    "ageh_data = df['ageh'].dropna()\n",
    "\n",
    "# Perform Gaussian Kernel Density Estimation\n",
    "kde = gaussian_kde(ageh_data)\n",
    "age_range = np.linspace(ageh_data.min(), ageh_data.max(), 1000)\n",
    "density_values = kde(age_range)\n",
    "\n",
    "# Find local minima\n",
    "minima_indices = find_peaks(-density_values)[0]\n",
    "minima_ages = age_range[minima_indices]\n",
    "\n",
    "# Plot density with minima\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(age_range, density_values, color='#4c72b0', lw=2, label='Density')\n",
    "plt.scatter(minima_ages, density_values[minima_indices], color='#e76f51', zorder=5, s=100, label='Minima (Split Points)')\n",
    "\n",
    "# Annotate minima\n",
    "for i, age in enumerate(minima_ages):\n",
    "    plt.text(age, density_values[minima_indices][i] + 0.002, f'{age:.2f}', fontsize=12, color='black', ha='center')\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Density Plot of Age of Household's Head with Minima\", fontsize=18, weight='bold', pad=20)\n",
    "plt.xlabel(\"Age of Household's Head\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "sns.despine()\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Print minima\n",
    "print(f\"Split points (minima) in the distribution: {minima_ages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'ageh' column and drop NaN values\n",
    "ageh_data = df['ageh'].dropna()\n",
    "\n",
    "# Perform Kernel Density Estimation\n",
    "kde = gaussian_kde(ageh_data)\n",
    "age_range = np.linspace(ageh_data.min(), ageh_data.max(), 1000)\n",
    "density_values = kde(age_range)\n",
    "\n",
    "# Identify local minima as split points\n",
    "minima_indices = find_peaks(-density_values)[0]\n",
    "minima_ages = age_range[minima_indices]\n",
    "\n",
    "# Determine the split point\n",
    "split_age = minima_ages[0]\n",
    "\n",
    "# Categorize ages into 'Young' and 'Old'\n",
    "df['age_group'] = np.where(df['ageh'] <= split_age, 'Young', 'Old')\n",
    "\n",
    "# Compute and rename summary statistics\n",
    "summary_stats = df.groupby('age_group')['ageh'].describe().rename(columns={\n",
    "    'count': 'Count',\n",
    "    'mean': 'Mean',\n",
    "    'std': 'Std Dev',\n",
    "    'min': 'Min',\n",
    "    '25%': '25%',\n",
    "    '50%': 'Median',\n",
    "    '75%': '75%',\n",
    "    'max': 'Max'\n",
    "})\n",
    "summary_stats.index.name = 'Age Group'\n",
    "\n",
    "# Display the summary statistics\n",
    "summary_stats.style.set_caption(\"Summary Statistics for Age Groups\").format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049cdfe",
   "metadata": {},
   "source": [
    "### Years of Education of Household Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Extract 'educh' data and drop NaN values\n",
    "educh_data = df['educh'].dropna()\n",
    "\n",
    "# Plot histogram of years of education\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(\n",
    "    educh_data,\n",
    "    bins=20,\n",
    "    kde=False,\n",
    "    color='#4c72b0',\n",
    "    edgecolor='white',\n",
    "    stat=\"percent\"\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Histogram of Years of Education\", fontsize=18, weight='bold', pad=20)\n",
    "plt.xlabel(\"Years of Education\", fontsize=14)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=14)\n",
    "sns.despine()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5791a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define buckets for education levels\n",
    "bins = [0, 4, 8, 16]\n",
    "labels = ['Low (0-4 years)', 'Medium (5-8 years)', 'High (9-16 years)']\n",
    "\n",
    "# Create 'education_bucket' column\n",
    "df['education_bucket'] = pd.cut(df['educh'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Generate summary statistics for each bucket\n",
    "summary_stats_buckets = df.groupby('education_bucket')['educh'].describe()\n",
    "\n",
    "# Rename index and columns\n",
    "summary_stats_buckets.rename(columns={\n",
    "    'count': 'Count',\n",
    "    'mean': 'Mean',\n",
    "    'std': 'Std Dev',\n",
    "    'min': 'Min',\n",
    "    '25%': '25%',\n",
    "    '50%': 'Median',\n",
    "    '75%': '75%',\n",
    "    'max': 'Max'\n",
    "}, inplace=True)\n",
    "summary_stats_buckets.index.name = 'Education Group'\n",
    "\n",
    "# Display\n",
    "summary_stats_buckets.style.set_caption(\"Summary Statistics for Education Buckets\").format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot histogram for education buckets with percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(\n",
    "    df['education_bucket'],\n",
    "    color='#4c72b0',\n",
    "    edgecolor='white',\n",
    "    shrink=0.8,\n",
    "    stat=\"percent\"\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Education Buckets\", fontsize=18, weight='bold', pad=20)\n",
    "plt.xlabel(\"Education Level Buckets\", fontsize=14)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=14)\n",
    "sns.despine()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1eadfc",
   "metadata": {},
   "source": [
    "### Farmer Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'farmer_status' column\n",
    "df['farmer_status'] = np.where(df['farm'] == 1, 'Farmer', 'Non-Farmer')\n",
    "\n",
    "# Summary statistics for farmers and non-farmers\n",
    "summary_stats_farmers = df.groupby('farmer_status')['farm'].describe()\n",
    "\n",
    "# Rename index and columns\n",
    "summary_stats_farmers.rename(columns={\n",
    "    'count': 'Count',\n",
    "    'mean': 'Mean',\n",
    "    'std': 'Std Dev',\n",
    "    'min': 'Min',\n",
    "    '25%': '25%',\n",
    "    '50%': 'Median',\n",
    "    '75%': '75%',\n",
    "    'max': 'Max'\n",
    "}, inplace=True)\n",
    "summary_stats_farmers.index.name = 'Farmer Status'\n",
    "\n",
    "# Display\n",
    "summary_stats_farmers.style.set_caption(\"Summary Statistics for Farmers and Non-Farmers\").format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'farmer_status' column\n",
    "df['farmer_status'] = np.where(df['farm'] == 1, 'Farmer', 'Non-Farmer')\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot histogram for farm status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(\n",
    "    df['farmer_status'],\n",
    "    color='#4c72b0',\n",
    "    edgecolor='white',\n",
    "    shrink=0.8,\n",
    "    stat=\"percent\"\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Farm Status\", fontsize=18, weight='bold', pad=20)\n",
    "plt.xlabel(\"Farm Status (Farmer vs Non-Farmer)\", fontsize=14)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=14)\n",
    "sns.despine()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf65632",
   "metadata": {},
   "source": [
    "### Finalize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261f232",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define required columns\n",
    "required_columns = [\n",
    "    'caseid', 'villageyear', 'age_group', 'education_bucket', 'farmer_status', 'year',\n",
    "    'invHHtvf1', 'invHHtvf2', 'madult', 'fadult', 'kids',\n",
    "    'maleh', 'farm', 'ageh', 'age2h', 'educh', 'vfst', 'newst'\n",
    "]\n",
    "\n",
    "# Filter necessary columns\n",
    "data = df[required_columns].copy()\n",
    "\n",
    "# Convert 'villageyear' to numeric\n",
    "data['villageyear'] = pd.to_numeric(data['villageyear'], errors='coerce')\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "data = pd.get_dummies(data, columns=['age_group', 'education_bucket', 'farmer_status'], drop_first=True)\n",
    "\n",
    "# Create year dummies\n",
    "data = pd.concat([data, pd.get_dummies(data['year'], prefix='year', drop_first=True)], axis=1)\n",
    "data.drop(columns=['year'], inplace=True)\n",
    "\n",
    "# Rename dummy variables\n",
    "rename_columns = {\n",
    "    'age_group_Young': 'Young',\n",
    "    'education_bucket_Medium (5-8 years)': 'Medium',\n",
    "    'education_bucket_High (9-16 years)': 'High',\n",
    "    'farmer_status_Non-Farmer': 'NonFarmer'\n",
    "}\n",
    "data.rename(columns=rename_columns, inplace=True)\n",
    "\n",
    "# Convert dummy variables to integers\n",
    "dummy_columns = ['Young', 'Medium', 'High', 'NonFarmer'] + [col for col in data.columns if col.startswith('year_')]\n",
    "data[dummy_columns] = data[dummy_columns].astype(int)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Set multi-index\n",
    "data['caseid'] = data['caseid'].astype('category')\n",
    "data['villageyear'] = data['villageyear'].astype(int)\n",
    "data = data.set_index(['caseid', 'villageyear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9cde2",
   "metadata": {},
   "source": [
    "## IV 2SLS Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453dd41",
   "metadata": {},
   "source": [
    "### First Stage Regression (IV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e0411",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the first-stage regression formula\n",
    "formula = \"\"\"\n",
    "vfst ~ invHHtvf1 + invHHtvf2 + madult + fadult + kids + maleh + farm + \n",
    "       ageh + age2h + educh + year_2 + year_3 + year_4 + year_5 + year_6 + year_7 + EntityEffects\n",
    "\"\"\"\n",
    "\n",
    "# Extract 'villageyear' for clustering\n",
    "clusters = data.index.get_level_values('villageyear').to_series(index=data.index)\n",
    "\n",
    "# Fit the PanelOLS model\n",
    "model = PanelOLS.from_formula(formula, data)\n",
    "firststage = model.fit(cov_type='clustered', clusters=clusters)\n",
    "\n",
    "# Add predicted values to the dataset\n",
    "data = data.copy()\n",
    "data['vfst_hat'] = firststage.fitted_values\n",
    "\n",
    "# Print regression summary\n",
    "print(firststage.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8dd899",
   "metadata": {},
   "source": [
    "### Second Stage Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b08777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction terms\n",
    "data['vfst_hat_Young'] = data['vfst_hat'] * data['Young']\n",
    "data['vfst_hat_Medium'] = data['vfst_hat'] * data['Medium']\n",
    "data['vfst_hat_High'] = data['vfst_hat'] * data['High']\n",
    "data['vfst_hat_NonFarmer'] = data['vfst_hat'] * data['NonFarmer']\n",
    "\n",
    "# Extract 'villageyear' as clusters\n",
    "clusters = data.index.get_level_values('villageyear').to_series(index=data.index)\n",
    "\n",
    "# Define regression formula\n",
    "formula = \"\"\"\n",
    "newst ~ vfst_hat_Young + vfst_hat_Medium + vfst_hat_High + vfst_hat_NonFarmer\n",
    "        + Young + Medium + High + NonFarmer\n",
    "        + madult + fadult + kids + maleh\n",
    "        + year_2 + year_3 + year_4 + year_5 + year_6 + year_7 + EntityEffects\n",
    "\"\"\"\n",
    "\n",
    "# Fit regression model with clustered standard errors\n",
    "model = PanelOLS.from_formula(formula, data, drop_absorbed=True)\n",
    "secondstage = model.fit(cov_type='clustered', clusters=clusters)\n",
    "\n",
    "# Print results\n",
    "print(secondstage.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e4621",
   "metadata": {},
   "source": [
    "### OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction terms for vfst and categorical groups\n",
    "data['vfst_Young'] = data['vfst'] * data['Young']\n",
    "data['vfst_Medium'] = data['vfst'] * data['Medium']\n",
    "data['vfst_High'] = data['vfst'] * data['High']\n",
    "data['vfst_NonFarmer'] = data['vfst'] * data['NonFarmer']\n",
    "\n",
    "# Extract 'villageyear' as clusters\n",
    "clusters = data.index.get_level_values('villageyear').to_series(index=data.index)\n",
    "\n",
    "# Define OLS regression formula\n",
    "formula = \"\"\"\n",
    "newst ~ vfst_Young + vfst_Medium + vfst_High + vfst_NonFarmer\n",
    "        + Young + Medium + High + NonFarmer\n",
    "        + madult + fadult + kids + maleh\n",
    "        + year_2 + year_3 + year_4 + year_5 + year_6 + year_7 + EntityEffects\n",
    "\"\"\"\n",
    "\n",
    "# Fit OLS model with clustered standard errors\n",
    "model = PanelOLS.from_formula(formula, data, drop_absorbed=True)\n",
    "ols = model.fit(cov_type='clustered', clusters=clusters)\n",
    "\n",
    "# Print results \n",
    "print(ols.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136941c",
   "metadata": {},
   "source": [
    "## Durbin–Wu–Hausman Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-stage regression (IV)\n",
    "formula_first_stage = \"\"\"\n",
    "vfst ~ invHHtvf1 + invHHtvf2 + madult + fadult + kids + maleh + farm +\n",
    "        ageh + age2h + educh + year_2 + year_3 + year_4 + year_5 + year_6 + year_7 + EntityEffects\n",
    "\"\"\"\n",
    "clusters = data.index.get_level_values('villageyear').to_series(index=data.index)\n",
    "model_first_stage = PanelOLS.from_formula(formula_first_stage, data)\n",
    "result_first_stage = model_first_stage.fit(cov_type='clustered', clusters=clusters)\n",
    "data['residuals_iv'] = result_first_stage.resids\n",
    "\n",
    "# Second-stage OLS regression with residuals\n",
    "formula_hausman = \"\"\"\n",
    "newst ~ vfst + residuals_iv + Young + Medium + High + NonFarmer\n",
    "        + madult + fadult + kids + maleh \n",
    "        + year_2 + year_3 + year_4 + year_5 + year_6 + year_7 + EntityEffects\n",
    "\"\"\"\n",
    "model_hausman = PanelOLS.from_formula(formula_hausman, data, drop_absorbed=True)\n",
    "result_hausman = model_hausman.fit(cov_type='clustered', clusters=clusters)\n",
    "\n",
    "# Hausman test\n",
    "try:\n",
    "    coeff_residuals = result_hausman.params['residuals_iv']\n",
    "    std_err_residuals = result_hausman.std_errors['residuals_iv']\n",
    "    hausman_stat = (coeff_residuals / std_err_residuals) ** 2\n",
    "    p_value = 1 - chi2.cdf(hausman_stat, df=1)\n",
    "\n",
    "    print(\"Hausman Test for Endogeneity:\")\n",
    "    print(f\"Hausman Statistic: {hausman_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"Reject the null hypothesis: Endogeneity detected. IV is necessary.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis: No endogeneity detected. OLS is consistent.\")\n",
    "except KeyError:\n",
    "    print(\"Hausman test could not be computed: 'residuals_iv' coefficient not found.\")\n",
    "\n",
    "# First-stage F-statistic\n",
    "print(\"\\nFirst-stage F-statistic:\")\n",
    "print(result_first_stage.f_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cf102",
   "metadata": {},
   "source": [
    "## DAG Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dag_iv_2sls():\n",
    "    dag = Digraph(comment=\"IV 2SLS Regression DAG\", format='png')\n",
    "    dag.node('IV', 'Inverse Village Size')\n",
    "    dag.node('PVFC', 'Predicted Village Fund Credit')\n",
    "    dag.node('Interaction', 'Predicted Village Fund Credit x Subgroup')\n",
    "    dag.node('NSC', 'New Short-Term Credit')\n",
    "    dag.node('Controls', 'Control Variables')\n",
    "    dag.edge('IV', 'PVFC', label=\" Instrument\")\n",
    "    dag.edge('PVFC', 'Interaction', label=\" Predicted Values\")\n",
    "    dag.edge('Interaction', 'NSC', xlabel=\"Interaction Effect        \")\n",
    "    dag.edge('Controls', 'NSC', label=\"Controls\")\n",
    "    return dag\n",
    "\n",
    "def create_dag_ols():\n",
    "    dag = Digraph(comment=\"OLS Regression DAG\", format='png')\n",
    "    dag.node('VFC', 'Village Fund Credit')\n",
    "    dag.node('Interaction', 'Village Fund Credit x Subgroup')\n",
    "    dag.node('NSC', 'New Short-Term Credit')\n",
    "    dag.node('Controls', 'Control Variables')\n",
    "    dag.edge('VFC', 'Interaction', label=\" Direct Effect\")\n",
    "    dag.edge('Interaction', 'NSC', xlabel=\"Interaction Effect         \")\n",
    "    dag.edge('Controls', 'NSC', label=\"Controls\")\n",
    "    return dag\n",
    "\n",
    "# Render DAGs\n",
    "dag_iv = create_dag_iv_2sls()\n",
    "dag_ols = create_dag_ols()\n",
    "\n",
    "dag_iv.render(filename='iv_2sls_dag', cleanup=True)\n",
    "dag_ols.render(filename='ols_dag', cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1479c4",
   "metadata": {},
   "source": [
    "## Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f010f",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c59c6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare feature matrix with renamed variables for clarity\n",
    "X = data[['vfst', 'madult', 'fadult', 'kids', 'maleh', 'farm', 'ageh', 'educh'] + \n",
    "         [col for col in data.columns if col.startswith('year_')]].copy()\n",
    "\n",
    "X.rename(columns={\n",
    "    'vfst': 'Village Fund Credit',\n",
    "    'madult': 'Number of Male Adults in Household',\n",
    "    'fadult': 'Number of Female Adults in Household',\n",
    "    'kids': 'Number of Kids in Household',\n",
    "    'maleh': 'Male Head of Household Dummy',\n",
    "    'ageh': 'Head of Household\\'s Age',\n",
    "    'educh': 'Head of Household\\'s Years of Education',\n",
    "    'farm': 'Occupation (Farmer vs. Non-Farmer)',\n",
    "    'year_2': 'Year 2',\n",
    "    'year_3': 'Year 3',\n",
    "    'year_4': 'Year 4',\n",
    "    'year_5': 'Year 5',\n",
    "    'year_6': 'Year 6',\n",
    "    'year_7': 'Year 7',\n",
    "}, inplace=True)\n",
    "\n",
    "# Define target variable\n",
    "y = data['newst']\n",
    "\n",
    "# Fit DecisionTreeRegressor\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = tree.predict(X)\n",
    "mse_tree = mean_squared_error(y, y_pred)\n",
    "rmse_tree = np.sqrt(mse_tree)\n",
    "r_squared_tree = r2_score(y, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Decision Tree Model Performance:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_tree:.4f}\")\n",
    "print(f\"R-squared: {r_squared_tree:.4f}\")\n",
    "\n",
    "# Plot decision tree\n",
    "plt.figure(figsize=(24, 12))\n",
    "plot_tree(\n",
    "    tree,\n",
    "    feature_names=X.columns,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=12,\n",
    "    precision=2,\n",
    "    impurity=False,\n",
    "    proportion=True\n",
    ")\n",
    "plt.title(\"Decision Tree for Predicting New Short-term Credit Level\", fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481758a",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336344b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize and fit RandomForestRegressor\n",
    "forest = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=0)\n",
    "forest.fit(X, y)\n",
    "\n",
    "# Predictions and evaluation metrics\n",
    "y_pred_forest = forest.predict(X)\n",
    "mse_forest = mean_squared_error(y, y_pred_forest)\n",
    "rmse_forest = np.sqrt(mse_forest)\n",
    "r_squared_forest = r2_score(y, y_pred_forest)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Random Forest - Mean Squared Error: {mse_forest}\")\n",
    "print(f\"Random Forest - Root Mean Squared Error: {rmse_forest}\")\n",
    "print(f\"Random Forest - R-squared: {r_squared_forest}\")\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': forest.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance Matrix:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance in Random Forest\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da305b7a",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GradientBoostingRegressor\n",
    "boosting_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=0)\n",
    "boosting_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_boosting = boosting_model.predict(X)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse_boosting = mean_squared_error(y, y_pred_boosting)\n",
    "rmse_boosting = np.sqrt(mse_boosting)\n",
    "r_squared_boosting = r2_score(y, y_pred_boosting)\n",
    "\n",
    "print(f\"Boosting - Mean Squared Error: {mse_boosting}\")\n",
    "print(f\"Boosting - Root Mean Squared Error: {rmse_boosting}\")\n",
    "print(f\"Boosting - R-squared: {r_squared_boosting}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importances_boosting = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': boosting_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nBoosting Feature Importance Matrix:\")\n",
    "print(feature_importances_boosting)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances_boosting['Feature'], feature_importances_boosting['Importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance in Gradient Boosting\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45121e46",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit BaggingRegressor\n",
    "bagging_model = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(max_depth=5),\n",
    "    n_estimators=100,\n",
    "    random_state=0\n",
    ")\n",
    "bagging_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bagging = bagging_model.predict(X)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse_bagging = mean_squared_error(y, y_pred_bagging)\n",
    "rmse_bagging = np.sqrt(mse_bagging)\n",
    "r_squared_bagging = r2_score(y, y_pred_bagging)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Bagging - Mean Squared Error: {mse_bagging}\")\n",
    "print(f\"Bagging - Root Mean Squared Error: {rmse_bagging}\")\n",
    "print(f\"Bagging - R-squared: {r_squared_bagging}\")\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = np.mean([tree.feature_importances_ for tree in bagging_model.estimators_], axis=0)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importance matrix\n",
    "print(\"\\nBagging Feature Importance Matrix (Averaged from Base Estimators):\")\n",
    "print(feature_importances_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance in Bagging\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994cd938",
   "metadata": {},
   "source": [
    "### Performance Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de0187",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Summarize error metrics for each model\n",
    "error_comparison = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Random Forest', 'Boosting', 'Bagging'],\n",
    "    'MSE': [mse_tree, mse_forest, mse_boosting, mse_bagging],\n",
    "    'RMSE': [rmse_tree, rmse_forest, rmse_boosting, rmse_bagging]\n",
    "})\n",
    "\n",
    "# Display the error comparison table\n",
    "print(\"\\nError Comparison Table:\")\n",
    "print(error_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cea165",
   "metadata": {},
   "source": [
    "### LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65893873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standardize X and y\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = (y - y.mean()) / y.std()\n",
    "\n",
    "# Fit LassoCV with cross-validation\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, random_state=0)\n",
    "lasso_cv.fit(X_scaled, y_scaled)\n",
    "\n",
    "# Cross-validated MSE vs. -log(alpha) plot\n",
    "mse_path_mean = lasso_cv.mse_path_.mean(axis=1)\n",
    "mse_path_std = lasso_cv.mse_path_.std(axis=1)\n",
    "log_alphas_cv = -np.log(lasso_cv.alphas_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(log_alphas_cv, mse_path_mean, yerr=mse_path_std, fmt='o-', ecolor='lightgray', capsize=3)\n",
    "plt.xlabel(r\"$-\\log(\\lambda)$\")\n",
    "plt.ylabel(\"Cross-validated MSE\")\n",
    "plt.title(\"Cross-validated MSE vs. -log(λ)\")\n",
    "plt.ylim(mse_path_mean.min() - 0.01, mse_path_mean.max() + 0.01)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Lasso Coefficients Path\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_scaled, y_scaled, alphas=np.logspace(-4, 1, 50))\n",
    "log_alphas_path = -np.log(alphas_lasso)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, feature in enumerate(X.columns):\n",
    "    plt.plot(log_alphas_path, coefs_lasso[i], label=feature)\n",
    "plt.xlabel(r\"$-\\log(\\lambda)$\")\n",
    "plt.ylabel(\"Standardized coefficients\")\n",
    "plt.title(\"Lasso Coefficients Path\")\n",
    "plt.legend(loc=\"upper right\", fontsize='small')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f786695",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standardize X and y\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = (y - y.mean()) / y.std()\n",
    "\n",
    "# Fit LassoCV with cross-validation\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, random_state=0)\n",
    "lasso_cv.fit(X_scaled, y_scaled)\n",
    "\n",
    "# Extract non-zero coefficients and corresponding features\n",
    "important_features = X.columns[lasso_cv.coef_ != 0]\n",
    "important_coefficients = lasso_cv.coef_[lasso_cv.coef_ != 0]\n",
    "\n",
    "# Create DataFrame for sorting and plotting\n",
    "important_features_df = pd.DataFrame({\n",
    "    'Feature': important_features,\n",
    "    'Coefficient': important_coefficients\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Plot important features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(important_features_df['Feature'], important_features_df['Coefficient'])\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Important Features Identified by LASSO\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59a878",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize X and y\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = (y - y.mean()) / y.std()\n",
    "\n",
    "# Define a range of alphas for Ridge\n",
    "alphas_ridge = np.logspace(-10, 10, 100)\n",
    "\n",
    "# Perform cross-validation for each alpha\n",
    "mse_mean, mse_std = [], []\n",
    "for alpha in alphas_ridge:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    neg_mse_scores = cross_val_score(ridge, X_scaled, y_scaled, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_scores = -neg_mse_scores\n",
    "    mse_mean.append(mse_scores.mean())\n",
    "    mse_std.append(mse_scores.std())\n",
    "\n",
    "# Convert to arrays for plotting\n",
    "mse_mean, mse_std = np.array(mse_mean), np.array(mse_std)\n",
    "log_alphas = -np.log(alphas_ridge)\n",
    "\n",
    "# Cross-validated MSE vs -log(alpha)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(log_alphas, mse_mean, yerr=mse_std, fmt='o-', ecolor='lightgray', capsize=3)\n",
    "plt.xlabel(r\"$-\\log(\\lambda)$\")\n",
    "plt.ylabel(\"Cross-validated MSE\")\n",
    "plt.title(\"Cross-validated MSE vs. -log(λ) for Ridge\")\n",
    "plt.ylim(mse_mean.min() - 0.0005, mse_mean.max() + 0.0005)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Ridge Coefficients Path\n",
    "coefs_ridge = []\n",
    "for alpha in alphas_ridge:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_scaled, y_scaled)\n",
    "    coefs_ridge.append(ridge.coef_)\n",
    "\n",
    "log_alphas_path = -np.log(alphas_ridge)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, feature in enumerate(X.columns):\n",
    "    plt.plot(log_alphas_path, [coef[i] for coef in coefs_ridge], label=feature)\n",
    "plt.xlabel(r\"$-\\log(\\lambda)$\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Ridge Coefficients Path\")\n",
    "plt.legend(loc=\"upper right\", fontsize='small')\n",
    "plt.xlim(log_alphas_path.min() - 2, log_alphas_path.max() + 2)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Important Features Plot\n",
    "best_alpha = alphas_ridge[np.argmin(mse_mean)]\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_scaled, y_scaled)\n",
    "\n",
    "important_features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': ridge_best.coef_\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(important_features_df['Feature'], important_features_df['Coefficient'])\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(f\"Feature Importance in Ridge Regression (Best λ = {best_alpha})\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0daefb",
   "metadata": {},
   "source": [
    "### DML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "data = data.copy()\n",
    "X = data[['invHHtvf1', 'invHHtvf2', 'madult', 'fadult', 'kids', 'maleh', 'farm',\n",
    "          'ageh', 'age2h', 'educh'] + [col for col in data.columns if col.startswith('year_')]]\n",
    "T = data['vfst']  # Treatment\n",
    "Y = data['newst']  # Outcome\n",
    "groups = ['Young', 'Medium', 'High', 'NonFarmer']\n",
    "residual_Y = np.zeros(len(Y))\n",
    "residual_T = np.zeros(len(T))\n",
    "\n",
    "# Cross-Fitting Residuals Using Machine Learning Models\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "    T_train, T_test = T.iloc[train_idx], T.iloc[test_idx]\n",
    "\n",
    "    # Hyperparameter tuning and fitting models\n",
    "    grid_y = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, n_jobs=-1)\n",
    "    grid_y.fit(X_train, Y_train)\n",
    "    best_y = grid_y.best_estimator_\n",
    "\n",
    "    grid_t = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, n_jobs=-1)\n",
    "    grid_t.fit(X_train, T_train)\n",
    "    best_t = grid_t.best_estimator_\n",
    "\n",
    "    residual_Y[test_idx] = Y_test - best_y.predict(X_test)\n",
    "    residual_T[test_idx] = T_test - best_t.predict(X_test)\n",
    "\n",
    "# Prepare Interaction Terms\n",
    "residual_T_df = pd.DataFrame({'residual_T': residual_T}, index=data.index)\n",
    "for group in groups:\n",
    "    residual_T_df[f'residual_T_{group}'] = residual_T_df['residual_T'] * data[group].values\n",
    "\n",
    "# Second Stage Regression\n",
    "interaction_cols = ['residual_T_' + g for g in groups]\n",
    "second_stage_X = residual_T_df[['residual_T'] + interaction_cols]\n",
    "second_stage_X = sm.add_constant(second_stage_X)\n",
    "ols_model = sm.OLS(residual_Y, second_stage_X).fit()\n",
    "\n",
    "# Display Results\n",
    "print(\"Second Stage Coefficients (Double Machine Learning):\")\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c132f60",
   "metadata": {},
   "source": [
    "### Deep IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data = data.copy()\n",
    "Z = data[['invHHtvf1', 'invHHtvf2']].values\n",
    "X = data[['madult', 'fadult', 'kids', 'maleh', 'farm', 'ageh', 'age2h', 'educh'] +\n",
    "         [col for col in data.columns if col.startswith('year_')]].values\n",
    "T = data['vfst'].values.reshape(-1, 1)\n",
    "Y = data['newst'].values.reshape(-1, 1)\n",
    "\n",
    "# Standardize covariates and instruments\n",
    "scaler_X, scaler_Z = StandardScaler(), StandardScaler()\n",
    "X_scaled, Z_scaled = scaler_X.fit_transform(X), scaler_Z.fit_transform(Z)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Z_train, Z_test, T_train, T_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Z_scaled, T, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Treatment and outcome models\n",
    "def create_treatment_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def create_outcome_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Calculate treatment effects\n",
    "def calculate_treatment_effects(X_test, outcome_model):\n",
    "    T0 = np.zeros((X_test.shape[0], 1))\n",
    "    T1 = np.ones((X_test.shape[0], 1))\n",
    "    Y_T0 = outcome_model.predict(np.hstack([X_test, T0]), verbose=0)\n",
    "    Y_T1 = outcome_model.predict(np.hstack([X_test, T1]), verbose=0)\n",
    "    return np.mean(Y_T1 - Y_T0), Y_T1 - Y_T0\n",
    "\n",
    "# Bootstrapping\n",
    "n_bootstraps = 1\n",
    "overall_effects = []\n",
    "group_effects = {group: [] for group in ['Young', 'Medium', 'High', 'NonFarmer']}\n",
    "\n",
    "for b in range(n_bootstraps):\n",
    "    indices = np.random.choice(range(X_train.shape[0]), size=X_train.shape[0], replace=True)\n",
    "    X_resampled, Z_resampled = X_train[indices], Z_train[indices]\n",
    "    T_resampled, Y_resampled = T_train[indices], Y_train[indices]\n",
    "\n",
    "    treatment_model = create_treatment_model(X_resampled.shape[1] + Z_resampled.shape[1])\n",
    "    treatment_model.fit(\n",
    "        np.hstack([Z_resampled, X_resampled]), T_resampled,\n",
    "        epochs=10, batch_size=32, verbose=0,\n",
    "        validation_split=0.1, callbacks=[EarlyStopping(patience=2, restore_best_weights=True)]\n",
    "    )\n",
    "    T_hat_resampled = treatment_model.predict(np.hstack([Z_resampled, X_resampled]), verbose=0)\n",
    "\n",
    "    outcome_model = create_outcome_model(X_resampled.shape[1] + 1)\n",
    "    outcome_model.fit(\n",
    "        np.hstack([X_resampled, T_hat_resampled]), Y_resampled,\n",
    "        epochs=10, batch_size=32, verbose=0,\n",
    "        validation_split=0.1, callbacks=[EarlyStopping(patience=2, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "    overall_effect, treatment_diff = calculate_treatment_effects(X_test, outcome_model)\n",
    "    overall_effects.append(overall_effect)\n",
    "\n",
    "    for group in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "        subgroup_indicator = data[group].iloc[:len(X_test)].values.reshape(-1, 1)\n",
    "        subgroup_effect = np.mean(treatment_diff * subgroup_indicator)\n",
    "        group_effects[group].append(subgroup_effect)\n",
    "\n",
    "# Confidence intervals\n",
    "def confidence_interval(effects):\n",
    "    lower, upper = np.percentile(effects, 2.5), np.percentile(effects, 97.5)\n",
    "    mean_effect = np.mean(effects)\n",
    "    return mean_effect, lower, upper\n",
    "\n",
    "mean_overall, lower_overall, upper_overall = confidence_interval(overall_effects)\n",
    "print(\"\\nDeep IV Estimated Treatment Effects with 95% Confidence Intervals:\")\n",
    "print(f\"Overall Treatment Effect: {mean_overall:.4f} (95% CI: [{lower_overall:.4f}, {upper_overall:.4f}])\")\n",
    "\n",
    "for group in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    mean_group, lower_group, upper_group = confidence_interval(group_effects[group])\n",
    "    print(f\"{group} Treatment Effect: {mean_group:.4f} (95% CI: [{lower_group:.4f}, {upper_group:.4f}])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead2096",
   "metadata": {},
   "source": [
    "### Binary Treatment Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fff62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create binary treatment variable\n",
    "data['vfst_binary'] = np.where(data['vfst'] > 0, 1, 0)\n",
    "\n",
    "# Verify treatment variable distribution\n",
    "print(\"Distribution of Binary Treatment Variable (vfst_binary):\")\n",
    "print(data['vfst_binary'].value_counts())\n",
    "\n",
    "# Create interaction terms for treatment and categorical groups\n",
    "data['vfst_binary_Young'] = data['vfst_binary'] * data['Young']\n",
    "data['vfst_binary_Medium'] = data['vfst_binary'] * data['Medium']\n",
    "data['vfst_binary_High'] = data['vfst_binary'] * data['High']\n",
    "data['vfst_binary_NonFarmer'] = data['vfst_binary'] * data['NonFarmer']\n",
    "\n",
    "# Extract 'villageyear' as clusters\n",
    "clusters = data.index.get_level_values('villageyear').to_series(index=data.index)\n",
    "\n",
    "# Define regression formula\n",
    "formula_binary = \"\"\"\n",
    "newst ~ vfst_binary_Young + vfst_binary_Medium + vfst_binary_High + vfst_binary_NonFarmer\n",
    "         + Young + Medium + High + NonFarmer\n",
    "         + madult + fadult + kids + maleh\n",
    "         + year_2 + year_3 + year_4 + year_5 + year_6 + year_7 + EntityEffects\n",
    "\"\"\"\n",
    "\n",
    "# Display first few rows with interaction terms\n",
    "print(\"First few rows with binary treatment and interaction terms:\")\n",
    "print(data[['vfst_binary', 'vfst_binary_Young', 'vfst_binary_Medium', 'vfst_binary_High', 'vfst_binary_NonFarmer']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36872c",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define covariates for PSM\n",
    "covariates = ['madult', 'fadult', 'kids', 'maleh', 'ageh', 'educh', 'farm'] + \\\n",
    "             [col for col in data.columns if col.startswith('year_')]\n",
    "X = data[covariates]\n",
    "y_treatment = data['vfst_binary']\n",
    "y_outcome = data['newst']\n",
    "\n",
    "# Estimate Propensity Scores\n",
    "logit = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "logit.fit(X, y_treatment)\n",
    "data['pscore'] = logit.predict_proba(X)[:, 1]\n",
    "\n",
    "# Nearest-Neighbor Matching\n",
    "treated = data[data['vfst_binary'] == 1]\n",
    "control = data[data['vfst_binary'] == 0]\n",
    "nn = NearestNeighbors(n_neighbors=1, metric='euclidean')\n",
    "nn.fit(control[['pscore']])\n",
    "distances, indices = nn.kneighbors(treated[['pscore']])\n",
    "\n",
    "matched_control = control.iloc[indices.flatten()].reset_index(drop=True)\n",
    "matched_treated = treated.reset_index(drop=True)\n",
    "matched_data = pd.concat([matched_treated, matched_control], axis=0)\n",
    "\n",
    "# Balance Check\n",
    "def standardized_mean_diff(var, group1, group2):\n",
    "    mean_diff = group1[var].mean() - group2[var].mean()\n",
    "    pooled_std = np.sqrt((group1[var].var() + group2[var].var()) / 2)\n",
    "    return mean_diff / pooled_std\n",
    "\n",
    "pre_match_smd = {var: standardized_mean_diff(var, treated, control) for var in covariates}\n",
    "post_match_smd = {var: standardized_mean_diff(var, matched_treated, matched_control) for var in covariates}\n",
    "\n",
    "# Estimate Overall ATE and Subgroup ATEs with Bootstrapping\n",
    "n_bootstraps = 1000\n",
    "ate_bootstrap = []\n",
    "subgroup_ates_bootstrap = {'Young': [], 'Medium': [], 'High': [], 'NonFarmer': []}\n",
    "\n",
    "def estimate_ate(treated_df, control_df):\n",
    "    return treated_df['newst'].mean() - control_df['newst'].mean()\n",
    "\n",
    "def estimate_subgroup_ate(subgroup_var, subgroup_value, treated_df, control_df):\n",
    "    treated_subgroup = treated_df[treated_df[subgroup_var] == subgroup_value]\n",
    "    control_subgroup = control_df[control_df[subgroup_var] == subgroup_value]\n",
    "    if len(treated_subgroup) > 0 and len(control_subgroup) > 0:\n",
    "        return treated_subgroup['newst'].mean() - control_subgroup['newst'].mean()\n",
    "    return np.nan\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    boot_treated = matched_treated.sample(frac=1, replace=True, random_state=i)\n",
    "    boot_control = matched_control.sample(frac=1, replace=True, random_state=i)\n",
    "    ate_bootstrap.append(estimate_ate(boot_treated, boot_control))\n",
    "    for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "        subgroup_ates_bootstrap[subgroup].append(\n",
    "            estimate_subgroup_ate(subgroup, 1, boot_treated, boot_control)\n",
    "        )\n",
    "\n",
    "def calculate_ci(boot_estimates):\n",
    "    return np.percentile(boot_estimates, [2.5, 97.5])\n",
    "\n",
    "# Report Results\n",
    "ate_mean = np.mean(ate_bootstrap)\n",
    "ate_ci = calculate_ci(ate_bootstrap)\n",
    "print(f\"Overall ATE: {ate_mean:.4f} (95% CI: {ate_ci[0]:.4f}, {ate_ci[1]:.4f})\")\n",
    "\n",
    "for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    subgroup_mean = np.mean(subgroup_ates_bootstrap[subgroup])\n",
    "    subgroup_ci = calculate_ci(subgroup_ates_bootstrap[subgroup])\n",
    "    print(f\"ATE for {subgroup}: {subgroup_mean:.4f} (95% CI: {subgroup_ci[0]:.4f}, {subgroup_ci[1]:.4f})\")\n",
    "\n",
    "# Visualize Propensity Score Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(treated['pscore'], bins=30, alpha=0.5, label='Treated', color='blue')\n",
    "plt.hist(control['pscore'], bins=30, alpha=0.5, label='Control', color='orange')\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('PSM Model: Propensity Score Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff306dcb",
   "metadata": {},
   "source": [
    "### IPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31252b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define treatment and covariates\n",
    "treatment = 'vfst_binary'\n",
    "covariates = ['madult', 'fadult', 'kids', 'maleh', 'ageh', 'educh', 'farm',\n",
    "              'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7']\n",
    "\n",
    "# Propensity score estimation using logistic regression\n",
    "logit = LogisticRegression(max_iter=1000)\n",
    "X = data[covariates]\n",
    "y = data[treatment]\n",
    "logit.fit(X, y)\n",
    "data['pscore'] = logit.predict_proba(X)[:, 1]\n",
    "\n",
    "# Calculate IPW weights\n",
    "data['ipw'] = np.where(data[treatment] == 1, 1 / data['pscore'], 1 / (1 - data['pscore']))\n",
    "\n",
    "# Calculate standardized mean difference\n",
    "def standardized_mean_difference(var, treatment, weights=None):\n",
    "    if weights is None:\n",
    "        treated_mean = data.loc[data[treatment] == 1, var].mean()\n",
    "        control_mean = data.loc[data[treatment] == 0, var].mean()\n",
    "        pooled_std = np.sqrt((data.loc[data[treatment] == 1, var].var() +\n",
    "                              data.loc[data[treatment] == 0, var].var()) / 2)\n",
    "    else:\n",
    "        treated_mean = (data.loc[data[treatment] == 1, var] * data.loc[data[treatment] == 1, weights]).sum() / \\\n",
    "                       data.loc[data[treatment] == 1, weights].sum()\n",
    "        control_mean = (data.loc[data[treatment] == 0, var] * data.loc[data[treatment] == 0, weights]).sum() / \\\n",
    "                       data.loc[data[treatment] == 0, weights].sum()\n",
    "        pooled_std = np.sqrt(0.5 * (data[var].std()**2))\n",
    "    return (treated_mean - control_mean) / pooled_std\n",
    "\n",
    "# Pre- and post-IPW balance\n",
    "print(\"Standardized Mean Differences Pre-IPW:\")\n",
    "for cov in covariates:\n",
    "    print(f\"{cov}: {standardized_mean_difference(cov, treatment):.4f}\")\n",
    "\n",
    "print(\"\\nStandardized Mean Differences Post-IPW:\")\n",
    "for cov in covariates:\n",
    "    print(f\"{cov}: {standardized_mean_difference(cov, treatment, 'ipw'):.4f}\")\n",
    "\n",
    "# Estimate ATE using weighted regression\n",
    "def estimate_ate(data, treatment, outcome, weights):\n",
    "    treated_outcome = (data[treatment] * data[outcome] * data[weights]).sum() / data[weights][data[treatment] == 1].sum()\n",
    "    control_outcome = ((1 - data[treatment]) * data[outcome] * data[weights]).sum() / data[weights][data[treatment] == 0].sum()\n",
    "    return treated_outcome - control_outcome\n",
    "\n",
    "# Bootstrap ATE estimates\n",
    "n_bootstraps = 1000\n",
    "ate_bootstrap = []\n",
    "subgroup_ates_bootstrap = {subgroup: [] for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']}\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    boot_data = data.sample(frac=1, replace=True, random_state=i)\n",
    "    ate_bootstrap.append(estimate_ate(boot_data, 'vfst_binary', 'newst', 'ipw'))\n",
    "    for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "        subgroup_data = boot_data[boot_data[subgroup] == 1]\n",
    "        if len(subgroup_data) > 0:\n",
    "            subgroup_ates_bootstrap[subgroup].append(\n",
    "                estimate_ate(subgroup_data, 'vfst_binary', 'newst', 'ipw')\n",
    "            )\n",
    "\n",
    "# Calculate confidence intervals\n",
    "def calculate_ci(bootstrap_estimates):\n",
    "    return np.percentile(bootstrap_estimates, [2.5, 97.5])\n",
    "\n",
    "# Report results\n",
    "overall_ate_mean = np.mean(ate_bootstrap)\n",
    "overall_ate_ci = calculate_ci(ate_bootstrap)\n",
    "print(f\"\\nOverall ATE: {overall_ate_mean:.4f} (95% CI: {overall_ate_ci[0]:.4f}, {overall_ate_ci[1]:.4f})\")\n",
    "\n",
    "for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    subgroup_mean = np.mean(subgroup_ates_bootstrap[subgroup])\n",
    "    subgroup_ci = calculate_ci(subgroup_ates_bootstrap[subgroup])\n",
    "    print(f\"ATE for {subgroup}: {subgroup_mean:.4f} (95% CI: {subgroup_ci[0]:.4f}, {subgroup_ci[1]:.4f})\")\n",
    "\n",
    "# Visualize propensity score distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data.loc[data[treatment] == 1, 'pscore'], bins=50, alpha=0.6, label='Treated', color='blue')\n",
    "plt.hist(data.loc[data[treatment] == 0, 'pscore'], bins=50, alpha=0.6, label='Control', color='orange')\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('IPW Model: Propensity Score Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4432d05",
   "metadata": {},
   "source": [
    "### MetaLearners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37de3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Suppress warnings and ensure reproducibility\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Prepare data\n",
    "data = data.copy()\n",
    "data['vfst_binary'] = np.where(data['vfst'] > 0, 1, 0)\n",
    "\n",
    "# Define variables\n",
    "X = ['madult', 'fadult', 'kids', 'maleh', 'ageh', 'educh', 'farm', \n",
    "     'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7']\n",
    "T = 'vfst_binary'\n",
    "Y = 'newst'\n",
    "subgroups = ['Young', 'Medium', 'High', 'NonFarmer']\n",
    "\n",
    "# Bootstrap function for 95% CIs\n",
    "def bootstrap_ate(learner_func, n_bootstrap=1):\n",
    "    ate_bootstrap = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample_data = data.sample(frac=1, replace=True, random_state=np.random.randint(1, 1_000_000))\n",
    "        ate_bootstrap.append(learner_func(sample_data))\n",
    "    lower = np.percentile(ate_bootstrap, 2.5)\n",
    "    upper = np.percentile(ate_bootstrap, 97.5)\n",
    "    return np.mean(ate_bootstrap), (lower, upper)\n",
    "\n",
    "# Helper function to train model\n",
    "def train_model(X_train, y_train, **kwargs):\n",
    "    model = LGBMRegressor(n_estimators=100, max_depth=5, verbose=-1, **kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# S-Learner\n",
    "def s_learner_ate(sample):\n",
    "    s_learner = train_model(sample[X + [T]], sample[Y])\n",
    "    s_cate = (s_learner.predict(sample[X].assign(**{T: 1})) - \n",
    "              s_learner.predict(sample[X].assign(**{T: 0})))\n",
    "    return np.mean(s_cate)\n",
    "\n",
    "mean_ate, ci = bootstrap_ate(s_learner_ate)\n",
    "print(f\"\\n--- S-Learner ---\\nOverall ATE (S-Learner): {mean_ate:.4f} (95% CI: {ci})\")\n",
    "\n",
    "for subgroup in subgroups:\n",
    "    def s_learner_subgroup(sample):\n",
    "        subgroup_data = sample[sample[subgroup] == 1]\n",
    "        s_learner = train_model(subgroup_data[X + [T]], subgroup_data[Y])\n",
    "        s_cate_subgroup = (s_learner.predict(subgroup_data[X].assign(**{T: 1})) - \n",
    "                           s_learner.predict(subgroup_data[X].assign(**{T: 0})))\n",
    "        return np.mean(s_cate_subgroup)\n",
    "    \n",
    "    mean_ate, ci = bootstrap_ate(s_learner_subgroup)\n",
    "    print(f\"ATE for {subgroup} (S-Learner): {mean_ate:.4f} (95% CI: {ci})\")\n",
    "\n",
    "# T-Learner\n",
    "def t_learner_ate(sample):\n",
    "    model_control = train_model(sample[sample[T] == 0][X], sample[sample[T] == 0][Y])\n",
    "    model_treated = train_model(sample[sample[T] == 1][X], sample[sample[T] == 1][Y])\n",
    "    t_cate = model_treated.predict(sample[X]) - model_control.predict(sample[X])\n",
    "    return np.mean(t_cate)\n",
    "\n",
    "mean_ate, ci = bootstrap_ate(t_learner_ate)\n",
    "print(f\"\\n--- T-Learner ---\\nOverall ATE (T-Learner): {mean_ate:.4f} (95% CI: {ci})\")\n",
    "\n",
    "for subgroup in subgroups:\n",
    "    def t_learner_subgroup(sample):\n",
    "        subgroup_data = sample[sample[subgroup] == 1]\n",
    "        model_control = train_model(subgroup_data[subgroup_data[T] == 0][X], subgroup_data[subgroup_data[T] == 0][Y])\n",
    "        model_treated = train_model(subgroup_data[subgroup_data[T] == 1][X], subgroup_data[subgroup_data[T] == 1][Y])\n",
    "        t_cate = model_treated.predict(subgroup_data[X]) - model_control.predict(subgroup_data[X])\n",
    "        return np.mean(t_cate)\n",
    "    \n",
    "    mean_ate, ci = bootstrap_ate(t_learner_subgroup)\n",
    "    print(f\"ATE for {subgroup} (T-Learner): {mean_ate:.4f} (95% CI: {ci})\")\n",
    "\n",
    "# X-Learner\n",
    "def x_learner_ate(sample):\n",
    "    model_control = train_model(sample[sample[T] == 0][X], sample[sample[T] == 0][Y])\n",
    "    model_treated = train_model(sample[sample[T] == 1][X], sample[sample[T] == 1][Y])\n",
    "    tau_control = model_treated.predict(sample[sample[T] == 0][X]) - sample[sample[T] == 0][Y]\n",
    "    tau_treated = sample[sample[T] == 1][Y] - model_control.predict(sample[sample[T] == 1][X])\n",
    "    model_tau_control = train_model(sample[sample[T] == 0][X], tau_control)\n",
    "    model_tau_treated = train_model(sample[sample[T] == 1][X], tau_treated)\n",
    "    logit = LogisticRegression(max_iter=500)\n",
    "    logit.fit(sample[X], sample[T])\n",
    "    pscore = logit.predict_proba(sample[X])[:, 1]\n",
    "    x_cate = (pscore * model_tau_control.predict(sample[X]) +\n",
    "              (1 - pscore) * model_tau_treated.predict(sample[X]))\n",
    "    return np.mean(x_cate)\n",
    "\n",
    "mean_ate, ci = bootstrap_ate(x_learner_ate)\n",
    "print(f\"\\n--- X-Learner ---\\nOverall ATE (X-Learner): {mean_ate:.4f} (95% CI: {ci})\")\n",
    "\n",
    "for subgroup in subgroups:\n",
    "    def x_learner_subgroup(sample):\n",
    "        subgroup_data = sample[sample[subgroup] == 1]\n",
    "        return x_learner_ate(subgroup_data)\n",
    "    \n",
    "    mean_ate, ci = bootstrap_ate(x_learner_subgroup)\n",
    "    print(f\"ATE for {subgroup} (X-Learner): {mean_ate:.4f} (95% CI: {ci})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b81e9",
   "metadata": {},
   "source": [
    "### Doubly Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d13f93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Suppress warnings and ensure reproducibility\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Prepare data\n",
    "data = data.copy()\n",
    "data['vfst_binary'] = np.where(data['vfst'] > 0, 1, 0)\n",
    "\n",
    "# Define variables\n",
    "X = ['madult', 'fadult', 'kids', 'maleh', 'ageh', 'educh', 'farm', \n",
    "     'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7']\n",
    "T = 'vfst_binary'\n",
    "Y = 'newst'\n",
    "subgroups = ['Young', 'Medium', 'High', 'NonFarmer']\n",
    "\n",
    "# Doubly Robust Estimator\n",
    "def doubly_robust(df, X, T, Y):\n",
    "    ps = LogisticRegression(max_iter=500, solver='lbfgs').fit(df[X], df[T]).predict_proba(df[X])[:, 1]\n",
    "    model_control = LGBMRegressor(n_estimators=100, max_depth=5, min_data_in_leaf=10, min_gain_to_split=0.01)\n",
    "    model_treated = LGBMRegressor(n_estimators=100, max_depth=5, min_data_in_leaf=10, min_gain_to_split=0.01)\n",
    "    model_control.fit(df[df[T] == 0][X], df[df[T] == 0][Y])\n",
    "    model_treated.fit(df[df[T] == 1][X], df[df[T] == 1][Y])\n",
    "    mu0 = model_control.predict(df[X])\n",
    "    mu1 = model_treated.predict(df[X])\n",
    "    dr_ate = (\n",
    "        np.mean(df[T] * (df[Y] - mu1) / ps + mu1) -\n",
    "        np.mean((1 - df[T]) * (df[Y] - mu0) / (1 - ps) + mu0)\n",
    "    )\n",
    "    return dr_ate\n",
    "\n",
    "# Bootstrap for Confidence Intervals\n",
    "def bootstrap_ate(n_bootstrap=1):\n",
    "    ate_bootstrap = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample_data = data.sample(frac=1, replace=True, random_state=np.random.randint(0, 10000))\n",
    "        ate_bootstrap.append(doubly_robust(sample_data, X, T, Y))\n",
    "    lower = np.percentile(ate_bootstrap, 2.5)\n",
    "    upper = np.percentile(ate_bootstrap, 97.5)\n",
    "    return np.mean(ate_bootstrap), (lower, upper)\n",
    "\n",
    "# Overall ATE\n",
    "mean_ate, ci = bootstrap_ate()\n",
    "print(f\"Overall ATE (Doubly Robust): {mean_ate:.4f} (95% CI: {ci})\")\n",
    "\n",
    "# Subgroup ATEs\n",
    "for subgroup in subgroups:\n",
    "    def doubly_robust_subgroup(sample):\n",
    "        subgroup_data = sample[sample[subgroup] == 1]\n",
    "        return doubly_robust(subgroup_data, X, T, Y)\n",
    "    mean_ate, ci = bootstrap_ate()\n",
    "    print(f\"ATE for {subgroup} (Doubly Robust): {mean_ate:.4f} (95% CI: {ci})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74212b5",
   "metadata": {},
   "source": [
    "### Causal Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9070736",
   "metadata": {},
   "source": [
    "#### Binary Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced38f3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "Y = data['newst'].values\n",
    "T = data['vfst_binary'].values\n",
    "X = data[['Young', 'Medium', 'High', 'NonFarmer', \n",
    "          'madult', 'fadult', 'kids', 'maleh', \n",
    "          'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7']].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit CausalForestDML\n",
    "causal_forest = CausalForestDML(\n",
    "    model_t=LassoCV(cv=5),\n",
    "    model_y=LassoCV(cv=5),\n",
    "    n_estimators=500,\n",
    "    min_samples_leaf=10,\n",
    "    max_depth=5,\n",
    "    random_state=123\n",
    ")\n",
    "causal_forest.fit(Y=Y, T=T, W=X_scaled, X=X_scaled)\n",
    "\n",
    "# Predict treatment effects\n",
    "treatment_effects = causal_forest.effect(X_scaled)\n",
    "data['treatment_effects'] = treatment_effects\n",
    "\n",
    "# Summarize treatment effects by subgroup\n",
    "print(\"Summary of Treatment Effects by Subgroup:\")\n",
    "for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    subgroup_effects = data.loc[data[subgroup] == 1, 'treatment_effects']\n",
    "    print(f\"\\nSubgroup: {subgroup}\")\n",
    "    print(subgroup_effects.describe())\n",
    "\n",
    "# Visualize treatment effect distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    subgroup_effects = data.loc[data[subgroup] == 1, 'treatment_effects']\n",
    "    plt.hist(subgroup_effects, bins=30, alpha=0.5, label=subgroup)\n",
    "\n",
    "plt.title(\"Binary Treatment: Treatment Effect Distribution by Subgroup\")\n",
    "plt.xlabel(\"Treatment Effect\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature importance\n",
    "feature_importances = causal_forest.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(['Young', 'Medium', 'High', 'NonFarmer', \n",
    "          'madult', 'fadult', 'kids', 'maleh', \n",
    "          'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7'],\n",
    "         feature_importances)\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Binary Treatment: Feature Importance from Causal Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a9779",
   "metadata": {},
   "source": [
    "#### Unpredicted Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af877313",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "Y = data['newst'].values  \n",
    "T = data['vfst'].values \n",
    "X = data[['Young', 'Medium', 'High', 'NonFarmer', \n",
    "          'madult', 'fadult', 'kids', 'maleh', \n",
    "          'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7']].values  # Features\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit the CausalForestDML model\n",
    "causal_forest = CausalForestDML(\n",
    "    model_t=LassoCV(cv=5),\n",
    "    model_y=LassoCV(cv=5),\n",
    "    n_estimators=500,\n",
    "    min_samples_leaf=10,\n",
    "    max_depth=5,\n",
    "    random_state=123,\n",
    ")\n",
    "causal_forest.fit(Y=Y, T=T, W=X_scaled, X=X_scaled)\n",
    "\n",
    "# Predict treatment effects\n",
    "treatment_effects = causal_forest.effect(X_scaled)\n",
    "data['treatment_effects'] = treatment_effects\n",
    "\n",
    "# Summarize treatment effects by subgroup\n",
    "print(\"Summary of Treatment Effects by Subgroup:\")\n",
    "for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    subgroup_effects = data.loc[data[subgroup] == 1, 'treatment_effects']\n",
    "    print(f\"\\nSubgroup: {subgroup}\")\n",
    "    print(subgroup_effects.describe())\n",
    "\n",
    "# Visualize treatment effect distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    subgroup_effects = data.loc[data[subgroup] == 1, 'treatment_effects']\n",
    "    plt.hist(subgroup_effects, bins=30, alpha=0.5, label=subgroup)\n",
    "\n",
    "plt.title(\"Treatment Effect Distribution by Subgroup\")\n",
    "plt.xlabel(\"Treatment Effect\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance visualization\n",
    "feature_importances = causal_forest.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(['Young', 'Medium', 'High', 'NonFarmer', \n",
    "          'madult', 'fadult', 'kids', 'maleh', \n",
    "          'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7'],\n",
    "         feature_importances)\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance from Causal Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234394da",
   "metadata": {},
   "source": [
    "#### Predicted Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c62d6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "Y = data['newst'].values\n",
    "T = data['vfst_hat'].values\n",
    "X = data[['Young', 'Medium', 'High', 'NonFarmer', \n",
    "          'madult', 'fadult', 'kids', 'maleh', \n",
    "          'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7']].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit the CausalForestDML model\n",
    "causal_forest = CausalForestDML(\n",
    "    model_t=LassoCV(cv=5),\n",
    "    model_y=LassoCV(cv=5),\n",
    "    n_estimators=500,\n",
    "    min_samples_leaf=10,\n",
    "    max_depth=5,\n",
    "    random_state=123,\n",
    ")\n",
    "causal_forest.fit(Y=Y, T=T, W=X_scaled, X=X_scaled)\n",
    "\n",
    "# Predict treatment effects\n",
    "treatment_effects = causal_forest.effect(X_scaled)\n",
    "data['treatment_effects'] = treatment_effects\n",
    "\n",
    "# Summarize treatment effects by subgroup\n",
    "print(\"Summary of Treatment Effects by Subgroup:\")\n",
    "for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    subgroup_effects = data.loc[data[subgroup] == 1, 'treatment_effects']\n",
    "    print(f\"\\nSubgroup: {subgroup}\")\n",
    "    print(subgroup_effects.describe())\n",
    "\n",
    "# Visualize treatment effect distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "for subgroup in ['Young', 'Medium', 'High', 'NonFarmer']:\n",
    "    subgroup_effects = data.loc[data[subgroup] == 1, 'treatment_effects']\n",
    "    plt.hist(subgroup_effects, bins=30, alpha=0.5, label=subgroup)\n",
    "\n",
    "plt.title(\"Treatment Effect Distribution by Subgroup\")\n",
    "plt.xlabel(\"Treatment Effect\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance visualization\n",
    "feature_importances = causal_forest.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(['Young', 'Medium', 'High', 'NonFarmer', \n",
    "          'madult', 'fadult', 'kids', 'maleh', \n",
    "          'year_2', 'year_3', 'year_4', 'year_5', 'year_6', 'year_7'],\n",
    "         feature_importances)\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance from Causal Forest\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
